# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ABPDyf-yPfZNexDFsPjzylUhyMIGwMag
"""

!pip install transformers torch gradio accelerate PyPDF2 python-docx

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import PyPDF2
import docx
import io
import re
from typing import List, Tuple
import gc

# Initialize the model and tokenizer
MODEL_NAME = "ibm-granite/granite-3.2-2b-instruct"

class StudyAssistant:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_loaded = False
        print(f"Using device: {self.device}")

    def load_model(self):
        """Load model with memory optimization for Colab"""
        try:
            print("Loading model...")
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            # Move model to device if not using device_map
            if not torch.cuda.is_available():
                self.model = self.model.to(self.device)

            # Add padding token if it doesn't exist
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.model_loaded = True
            print("Model loaded successfully!")
            return "‚úÖ Model loaded successfully!"
        except Exception as e:
            print(f"Error loading model: {e}")
            self.model_loaded = False
            return f"‚ùå Error loading model: {e}"

    def generate_response(self, prompt: str, max_length: int = 512) -> str:
        """Generate response with memory management"""
        if not self.model_loaded or self.model is None:
            return "Please load the model first using the 'Load Model' button."

        try:
            # Ensure device is properly set
            device = "cuda" if torch.cuda.is_available() else "cpu"

            # Encode input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,
                padding=True
            )

            # Move inputs to the same device as model
            if torch.cuda.is_available() and hasattr(self.model, 'device'):
                device = self.model.device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3
                )

            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract only the generated part
            input_text = self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
            response = response[len(input_text):].strip()

            # Clean up GPU memory
            del inputs, outputs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return response if response else "I apologize, but I couldn't generate a proper response. Please try rephrasing your question."

        except Exception as e:
            print(f"Generation error: {e}")
            return f"Error generating response: {e}"

# Initialize assistant
assistant = StudyAssistant()

def concept_explanation(concept: str, age_group: str) -> str:
    """Explain concept based on age group"""
    age_prompts = {
        "5-8 years": "Explain like I'm 5 years old, using simple words and fun examples",
        "9-12 years": "Explain for elementary school students with relatable examples",
        "13-17 years": "Explain for teenagers with practical applications",
        "18+ years": "Provide a comprehensive explanation with detailed examples"
    }

    age_instruction = age_prompts.get(age_group, age_prompts["18+ years"])

    prompt = f"""
    {age_instruction}.

    Topic: {concept}

    Please provide a clear explanation with examples:
    """

    return assistant.generate_response(prompt, max_length=600)

def difference_explanation(concept1: str, concept2: str) -> str:
    """Explain differences between concepts"""
    prompt = f"""
    Compare and contrast these two concepts:
    Concept 1: {concept1}
    Concept 2: {concept2}

    Please provide:
    1. Key differences
    2. Similarities (if any)
    3. When to use each
    4. Examples for clarity
    """

    return assistant.generate_response(prompt, max_length=700)

def what_if_simulator(scenario: str) -> str:
    """Simulate hypothetical scenarios"""
    prompt = f"""
    Hypothetical scenario: {scenario}

    Please analyze this scenario and provide:
    1. Most likely outcome
    2. Alternative outcomes
    3. Factors that could influence the result
    4. Real-world implications
    """

    return assistant.generate_response(prompt, max_length=700)

def neuro_speed_mode(concept: str, speed: str) -> str:
    """Adjust explanation based on learning speed"""
    speed_prompts = {
        "Slow & Detailed": "Provide a very detailed, step-by-step explanation with multiple examples and analogies",
        "Moderate": "Give a balanced explanation with key points and relevant examples",
        "Fast & Concise": "Provide a quick, concise explanation focusing on essential points only"
    }

    speed_instruction = speed_prompts.get(speed, speed_prompts["Moderate"])

    prompt = f"""
    Learning Speed: {speed}
    {speed_instruction}

    Concept: {concept}
    """

    return assistant.generate_response(prompt, max_length=800)

def extract_text_from_pdf(file) -> str:
    """Extract text from PDF file"""
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text
    except Exception as e:
        return f"Error reading PDF: {e}"

def extract_text_from_docx(file) -> str:
    """Extract text from DOCX file"""
    try:
        doc = docx.Document(file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        return f"Error reading DOCX: {e}"

def reverse_engineering_exams(file, subject: str) -> str:
    """Analyze past papers and predict questions"""
    if file is None:
        return "Please upload a file first."

    # Extract text based on file type
    if file.name.endswith('.pdf'):
        text = extract_text_from_pdf(file)
    elif file.name.endswith('.docx'):
        text = extract_text_from_docx(file)
    else:
        return "Unsupported file type. Please upload PDF or DOCX files."

    if "Error" in text:
        return text

    # Truncate text if too long
    text = text[:2000] if len(text) > 2000 else text

    prompt = f"""
    Subject: {subject}

    Based on this past exam paper content:
    {text}

    Please predict likely exam questions by analyzing:
    1. Common question patterns
    2. Frequently tested topics
    3. Question formats used
    4. Suggested practice questions
    """

    return assistant.generate_response(prompt, max_length=800)

def ai_doppelganger(concept: str, hobbies: str) -> str:
    """Create AI study partner based on user's hobbies"""
    prompt = f"""
    Create an AI study partner who shares these hobbies: {hobbies}

    The AI partner should explain this concept: {concept}

    Please:
    1. Give the AI partner a personality based on the hobbies
    2. Explain the concept using analogies from those hobbies
    3. Make it engaging and relatable
    4. Include study tips related to the hobbies
    """

    return assistant.generate_response(prompt, max_length=800)

# Create Gradio interface
with gr.Blocks(title="AI Study Assistant", theme=gr.themes.Soft()) as app:
    gr.Markdown("# üß† AI Study Assistant")
    gr.Markdown("Powered by IBM Granite 3.2-2B model")

    # Model loading section
    with gr.Row():
        load_btn = gr.Button("üöÄ Load Model", variant="primary", scale=1)
        model_status = gr.Textbox(label="Model Status", interactive=False, scale=2)

    load_btn.click(fn=assistant.load_model, outputs=model_status)

    gr.Markdown("---")

    # Create tabs for different functionalities
    with gr.Tabs():

        # Tab 1: Concept Explanation
        with gr.Tab("üìñ Concept Explanation"):
            with gr.Row():
                concept_input = gr.Textbox(label="Enter Concept", placeholder="e.g., Photosynthesis")
                age_group = gr.Dropdown(
                    choices=["5-8 years", "9-12 years", "13-17 years", "18+ years"],
                    label="Age Group",
                    value="18+ years"
                )
            concept_btn = gr.Button("Explain Concept", variant="primary")
            concept_output = gr.Textbox(label="Explanation", lines=10)

            concept_btn.click(
                fn=concept_explanation,
                inputs=[concept_input, age_group],
                outputs=concept_output
            )

        # Tab 2: Difference Explanation
        with gr.Tab("üîç Difference Explanation"):
            with gr.Row():
                concept1_input = gr.Textbox(label="First Concept", placeholder="e.g., Mitosis")
                concept2_input = gr.Textbox(label="Second Concept", placeholder="e.g., Meiosis")
            diff_btn = gr.Button("Compare Concepts", variant="primary")
            diff_output = gr.Textbox(label="Comparison", lines=10)

            diff_btn.click(
                fn=difference_explanation,
                inputs=[concept1_input, concept2_input],
                outputs=diff_output
            )

        # Tab 3: What-If Simulator
        with gr.Tab("ü§î What-If Simulator"):
            scenario_input = gr.Textbox(
                label="Hypothetical Scenario",
                placeholder="What if gravity was half as strong?",
                lines=3
            )
            whatif_btn = gr.Button("Analyze Scenario", variant="primary")
            whatif_output = gr.Textbox(label="Analysis", lines=10)

            whatif_btn.click(
                fn=what_if_simulator,
                inputs=scenario_input,
                outputs=whatif_output
            )

        # Tab 4: Neuro Speed Mode
        with gr.Tab("‚ö° Neuro Speed Mode"):
            with gr.Row():
                neuro_concept = gr.Textbox(label="Concept", placeholder="e.g., Quantum Physics")
                learning_speed = gr.Dropdown(
                    choices=["Slow & Detailed", "Moderate", "Fast & Concise"],
                    label="Learning Speed",
                    value="Moderate"
                )
            neuro_btn = gr.Button("Generate Explanation", variant="primary")
            neuro_output = gr.Textbox(label="Customized Explanation", lines=10)

            neuro_btn.click(
                fn=neuro_speed_mode,
                inputs=[neuro_concept, learning_speed],
                outputs=neuro_output
            )

        # Tab 5: Reverse Engineering Exams
        with gr.Tab("üìù Exam Question Predictor"):
            file_upload = gr.File(
                label="Upload Past Paper",
                file_types=[".pdf", ".docx"],
                type="filepath"
            )
            subject_input = gr.Textbox(label="Subject", placeholder="e.g., Biology, Mathematics")
            exam_btn = gr.Button("Predict Questions", variant="primary")
            exam_output = gr.Textbox(label="Predicted Questions", lines=10)

            exam_btn.click(
                fn=reverse_engineering_exams,
                inputs=[file_upload, subject_input],
                outputs=exam_output
            )

        # Tab 6: AI Doppelg√§nger
        with gr.Tab("üë• AI Study Partner"):
            with gr.Row():
                doppel_concept = gr.Textbox(label="Concept to Learn", placeholder="e.g., Machine Learning")
                hobbies_input = gr.Textbox(
                    label="Your Hobbies",
                    placeholder="e.g., Gaming, Music, Sports",
                    lines=2
                )
            doppel_btn = gr.Button("Create Study Partner", variant="primary")
            doppel_output = gr.Textbox(label="Your AI Study Partner", lines=12)

            doppel_btn.click(
                fn=ai_doppelganger,
                inputs=[doppel_concept, hobbies_input],
                outputs=doppel_output
            )

    # Footer
    gr.Markdown("---")
    gr.Markdown("üí° **Tip:** Load the model first, then explore different study modes!")

# Launch the app
if __name__ == "__main__":
    app.queue(max_size=10)  # Enable queuing for better stability
    app.launch(
        share=True,
        debug=False,
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        auth=None  # Remove if you want to add authentication
    )

